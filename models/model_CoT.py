# coding=utf-8
from __future__ import absolute_import
from __future__ import division
from __future__ import print_function

import logging
from os.path import join as pjoin

import torch
from torch.nn import CrossEntropyLoss, Dropout, Softmax, Linear, Conv2d, LayerNorm
from torch_geometric.nn import GENConv, DeepGCNLayer
from torch.nn import ReLU
import models.configs as configs
from .model_utils import *
import ml_collections
import random

def get_t16_config():
    """Returns the ViT-T/16 configuration."""
    config = ml_collections.ConfigDict()
    config.patches = ml_collections.ConfigDict({'size': (16, 16)})
    config.split = 'non-overlap'
    config.slide_step = 12
    config.input_size = 1024
    config.hidden_size = 192
    config.transformer = ml_collections.ConfigDict()
    config.transformer.mlp_dim = 192*4
    config.transformer.num_heads = 3
    config.transformer.num_layers = 1
    config.transformer.attention_dropout_rate = 0.0
    config.transformer.dropout_rate = 0.1
    config.classifier = 'token'
    config.representation_size = None
    return config

def _no_grad_trunc_normal_(tensor, mean, std, a, b):
    # Cut & paste from PyTorch official master until it's in a few official releases - RW
    # Method based on https://people.sc.fsu.edu/~jburkardt/presentations/truncated_normal.pdf
    def norm_cdf(x):
        # Computes standard normal cumulative distribution function
        return (1. + math.erf(x / math.sqrt(2.))) / 2.

    if (mean < a - 2 * std) or (mean > b + 2 * std):
        warnings.warn("mean is more than 2 std from [a, b] in nn.init.trunc_normal_. "
                      "The distribution of values may be incorrect.",
                      stacklevel=2)

    with torch.no_grad():
        # Values are generated by using a truncated uniform distribution and
        # then using the inverse CDF for the normal distribution.
        # Get upper and lower cdf values
        l = norm_cdf((a - mean) / std)
        u = norm_cdf((b - mean) / std)

        # Uniformly fill tensor with values from [l, u], then translate to
        # [2l-1, 2u-1].
        tensor.uniform_(2 * l - 1, 2 * u - 1)

        # Use inverse cdf transform for normal distribution to get truncated
        # standard normal
        tensor.erfinv_()

        # Transform to proper mean, std
        tensor.mul_(std * math.sqrt(2.))
        tensor.add_(mean)

        # Clamp to ensure it's in the proper range
        tensor.clamp_(min=a, max=b)
        return tensor


def trunc_normal_(tensor, mean=0., std=1., a=-2., b=2.):
    # type: (Tensor, float, float, float, float) -> Tensor
    r"""Fills the input Tensor with values drawn from a truncated
    normal distribution. The values are effectively drawn from the
    normal distribution :math:`\mathcal{N}(\text{mean}, \text{std}^2)`
    with values outside :math:`[a, b]` redrawn until they are within
    the bounds. The method used for generating the random values works
    best when :math:`a \leq \text{mean} \leq b`.
    Args:
        tensor: an n-dimensional `torch.Tensor`
        mean: the mean of the normal distribution
        std: the standard deviation of the normal distribution
        a: the minimum cutoff value
        b: the maximum cutoff value
    Examples:
        >>> w = torch.empty(3, 5)
        >>> nn.init.trunc_normal_(w)
    """
    return _no_grad_trunc_normal_(tensor, mean, std, a, b)

class attention_block(nn.Module):
    def __init__(self, config):
        super(attention_block, self).__init__()
        self.L = 1024
        self.D = config.hidden_size
        self.K = 1
        self.attention_V = nn.Sequential(nn.Linear(self.L, self.D), nn.Tanh())
        self.attention_U = nn.Sequential(nn.Linear(self.L, self.D), nn.Sigmoid())
        self.attention_weights = nn.Linear(self.D, self.K)
    def forward(self, x):
        H = x.float()
        A_V = self.attention_V(H)  # NxD
        A_U = self.attention_U(H)  # NxD
        A = self.attention_weights(A_V * A_U) # element wise multiplication # NxK
        A = torch.transpose(A, 1, 0)  # KxN
        A = F.softmax(A, dim=1)  # softmax over N
        # M = torch.mm(A, H)  # KxL
        M = A.T * H
        return M, A # updated patch feature, attention map


class CoT(nn.Module):
    def __init__(self, config, img_size=224, num_classes=21843, smoothing_value=0, zero_head=False):
        super(CoT, self).__init__()
        self.loss_ce = nn.CrossEntropyLoss()
        self.L = 1024
        self.K = 1
        self.classifier = nn.Sequential(nn.Linear(self.L*self.K, num_classes))

        # FOR TEXT FEATURE
        self.attention_block = attention_block(config)
        self.fc_text = nn.Sequential(nn.Linear(self.L, self.L), nn.ReLU(), nn.Linear(self.L, self.L))

        # for ablation
        self.self_attention = MultiheadAttention(embed_dim=1024, num_heads=1)

    def forward(self, x_s, coord_s, x_l, coords_l, patch_label, label, text_prompt_feature, slide_id, attention_only=False, labels=None):
        map = torch.load('map_10x_20x_files/'+''.join([chr(int(item)) for item in slide_id]).rstrip('.tif')+'.pt')

        H = x_s.float()
        M, A = self.attention_block(H)

        text_feature = self.fc_text(text_prompt_feature.float())
        logits_text_low = text_feature[:2] @ M.T # [:2] for tcga_lung [:3]
        sim_map = logits_text_low
        logits_text_low = torch.sum(logits_text_low.T, dim=0).unsqueeze(0)

        A = sim_map[torch.topk(logits_text_low, 1, dim = 1)[1].squeeze()].unsqueeze(0)
        _, mask_id = torch.topk(A, int(A.shape[1] * 0.1))
        mask = torch.ones_like(A, dtype=torch.bool).squeeze()
        mask[mask_id.squeeze()] = False
        M_back = M[mask]
        sim_back = torch.sum(M_back @ torch.cat((text_feature[:2], text_feature[4:-1]), dim=0).T, dim=0).unsqueeze(0)  # [:2] [4:-1] ; [:3] [6:-1]
        loss_cl = self.loss_ce(sim_back, label)

        _, top_indices = torch.topk(A, min(A.shape[1], 16)) 
        high_scale_ids = map[top_indices.squeeze()].flatten()
        high_scale_ids = high_scale_ids[high_scale_ids!=-1]
        selected_high_scale_patch = x_l.float()[high_scale_ids.type(torch.long)]
        high_M, _ = self.attention_block(selected_high_scale_patch)
        logits_text_high = torch.sum(high_M @ text_feature[2:4].T, dim=0).unsqueeze(0)  # [2:4]; [3:6]


        logits = logits_text_low + logits_text_high
        loss = self.loss_ce(logits, label) + 1 * loss_cl

        Y_prob = F.softmax(logits, dim = 1)
        Y_hat = torch.topk(logits, 1, dim = 1)[1]

        patch_prediction = (A - A.min()) / (A.max() - A.min())
        patch_prediction[patch_prediction > 0.5] = 1
        patch_prediction[patch_prediction != 1] = 0

        return logits, Y_prob, Y_hat, patch_prediction, A, loss


CONFIGS = {
    'CoT': configs.get_t16_config(),
}